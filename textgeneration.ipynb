{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d280cd4c-2085-4908-bdc7-57bb3051bcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81de9fe6-79c8-44e5-bb8e-0f6b4bab6d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1115394/1115394 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d06ac06-1c75-49cb-8738-1bd49c4fcceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30c60b98-2776-48e6-9f8c-0ff3fb71fafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "591d05ce-83f5-4237-a6dd-f9440c1fbceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d6659b-bb37-4f48-9494-ff988af92e68",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7783747-5598-4725-8a7c-bcee63409578",
   "metadata": {},
   "source": [
    "*From Tensorflow:*\n",
    "\n",
    "The `tf.keras.layers.StringLookup` layer can convert each character into a numeric ID.\n",
    "\n",
    "**But** it needs the text to be split into tokens first. For that we can use the split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "923b167f-6531-4402-9a4a-6190a3864f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b't', b'e', b's', b't', b'e']]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.unicode_split(['teste'], input_encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55a8e7d9-b631-4ad8-9b3d-ce46b036699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numeric ID\n",
    "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
    "# convert back to text\n",
    "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "\n",
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1) # just join the chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9c8cb1c-ee14-4aa9-8824-f5c55d2b13a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1], dtype=int64)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e1eaf69-4b5b-49f3-833b-4832781e9090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn it into a dataset\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e413d490-8e2d-429f-aef4-3b8540875337",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100 # input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05661a56-4c17-4210-9f0c-7c4beff4b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c5b36a-5d73-48b2-a5ab-802a86a7cda6",
   "metadata": {},
   "source": [
    "Example of 2 inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c26d911-3728-4ca5-95cc-51d2a2eb5eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(2):\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174c71b5-2584-40ab-8b5a-8259de910a17",
   "metadata": {},
   "source": [
    "## Prepare input and output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f970b3b0-399b-4cf7-ae82-e154e4266134",
   "metadata": {},
   "source": [
    "If the input is **'Hell'** and the output should be **'ello'**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a85c92a3-b306-464f-b705-1faac940486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d40330c0-5374-40ec-b4cc-ef932d55d23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['H', 'e', 'l', 'l'], ['e', 'l', 'l', 'o'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"Hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f639fc7-4548-49b6-8071-8a499d909c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a25480f-eea4-4379-886d-ba9d10f7444d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "Input : b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you '\n",
      "Target: b're all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(2):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c3cc97-99f3-4b3f-bda9-0073152568ae",
   "metadata": {},
   "source": [
    "## Batch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed7cead0-4c4b-4104-be85-a77cd6898798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000 # dont try to shuffle everything at once\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cd002f-b476-469b-95ef-21a7478c6c79",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1c57268-4e65-496d-a683-0b4221cb1a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ea30b4-fdb6-4b26-8cf1-115dc60ad3ac",
   "metadata": {},
   "source": [
    "The model uses a **GRU** to learn the encoding, the `state` has to be returned in order to feed the next input (iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5492f98-d6b0-4dd8-a99e-5dad9893cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a2d2fb3-220e-4ecf-9a18-bec1336e9b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(vocab_size=vocab_size, embedding_dim=embedding_dim, rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e1447d-80d8-44bf-8bbf-b1acb5afdef6",
   "metadata": {},
   "source": [
    "The problem can be treated as a standard **classification** problem.\n",
    "\n",
    "Given the **previous** RNN state, and the **input** this time step, predict the class of the **next** character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebf5f9f2-06ff-4800-8cb2-ac8a67844646",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5b9d35-ece4-4974-a05d-27b2df8e99a3",
   "metadata": {},
   "source": [
    "**Important**:\n",
    "*From Tensorflow:*\n",
    "\n",
    "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes.\n",
    "\n",
    "To confirm this you can check that the exponential of the mean loss is **approximately equal** to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb694351-e80a-4b5f-9817-18883ce63c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.1887693, shape=(), dtype=float32)\n",
      "Exp. mean loss:  65.94159\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "    \n",
    "    print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "    print(\"Mean loss:        \", example_batch_mean_loss)\n",
    "    print(\"Exp. mean loss: \", tf.exp(example_batch_mean_loss).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2eedf008-3c77-4dfd-9cc6-ad0b42dd9bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fa15b0d-3a38-4a1e-a6ba-f01c61e12c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     multiple                  16896     \n",
      "                                                                 \n",
      " gru_1 (GRU)                 multiple                  3938304   \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,022,850\n",
      "Trainable params: 4,022,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6d2ea78-94e7-48e2-9027-b5494dcfb979",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4d26c54-bc8c-45d7-afef-0bbf946dd538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "172/172 [==============================] - 309s 2s/step - loss: 1.4889\n",
      "Epoch 2/5\n",
      "172/172 [==============================] - 313s 2s/step - loss: 1.4075\n",
      "Epoch 3/5\n",
      "172/172 [==============================] - 310s 2s/step - loss: 1.3496\n",
      "Epoch 4/5\n",
      "172/172 [==============================] - 311s 2s/step - loss: 1.3001\n",
      "Epoch 5/5\n",
      "172/172 [==============================] - 307s 2s/step - loss: 1.2576\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b89288e-780e-4f64-815d-d70f8d0c5c9d",
   "metadata": {},
   "source": [
    "# Generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa37c346-5df8-49f6-bc96-fffc6b407ab9",
   "metadata": {},
   "source": [
    "*From Tensorflow:*\n",
    "\n",
    "The simplest way to generate text with this model is to run it in a loop, and keep track of the model's internal state as you execute it.\n",
    "\n",
    "Each time you call the model you pass in some text and an internal state.\n",
    "\n",
    "The model returns a prediction for the next character and its new state. Pass the prediction and state back in to continue generating text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ce6abfe-705f-45b7-afac-bfe85cecbe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    \n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            values=[-float('inf')]*len(skip_ids), # Put a -inf at each bad index.\n",
    "            indices=skip_ids,\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())] # Match the shape to the vocabulary\n",
    "        )\n",
    "    \n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        # Convert strings to token IDs.\n",
    "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        # Run the model.\n",
    "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "        predicted_logits, states = self.model(inputs=input_ids, states=states, return_state=True)\n",
    "        \n",
    "        # Only use the last prediction.\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits/self.temperature\n",
    "        \n",
    "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        # Sample the output logits to generate token IDs.\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "        # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "        # Return the characters and model state.\n",
    "        return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51614d89-9b86-43a3-a382-d61c8f4f4db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3bf74ecf-a20d-4c26-bf44-77b252bca81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "So, Gentlemen,\n",
      "Who take't, they must conferr revolt\n",
      "Shall knock it is with a foot.\n",
      "\n",
      "Provost:\n",
      "Northough no doubt, invocudious; which true banish'd,\n",
      "hereaft reign on ease suspicions.\n",
      "\n",
      "HARWINGHAM:\n",
      "Learn boot, an angly--under, you so understoo\n",
      "pursued in ectim and my trage is not of our house;\n",
      "Princely guid! my party deep by traveliament with me\n",
      "And I the haste seems' creature there thou returned?\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Let's golding then can speak of sellows, bresh'd up with him.\n",
      "Ah that we have drum'd the flatterer thrust of them;\n",
      "And I thou hast disnibe to death, seal me, deliver'd\n",
      "Where is Richard'd while I were go for, my lord,\n",
      "Than if you think, since it was tirdd his monatrous,\n",
      "Fortenst the golden bed of what\n",
      "they lead the modaltices through. What's thou? This fall twict\n",
      "What's a killet--lost Henry's nemble;\n",
      "I do not warrant-full flight: add more\n",
      "On my body sits of with these death,\n",
      "So may would have shed for you.\n",
      "\n",
      "AUTOLYCUS:\n",
      "Ah, Cares you, by their own, since, 'gainst their\n",
      "anviciorable \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.464669704437256\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022ab560-e040-4055-80e8-7c3cba86b55c",
   "metadata": {},
   "source": [
    "**Important:**\n",
    "\n",
    "*From Tensorflow:*\n",
    "\n",
    "The easiest thing you can do to improve the results is to train it for longer (try `EPOCHS = 30`).\n",
    "\n",
    "If you want the model to generate text faster the easiest thing you can do is batch the text generation:\n",
    "```\n",
    "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb7e8fd-7ea7-4556-a777-733785e2b7e4",
   "metadata": {},
   "source": [
    "# Advanced: Customized Training\n",
    "\n",
    "*From Tensorflow:*\n",
    "\n",
    "The above training procedure is simple, but does not give you much control. It uses **teacher-forcing** which prevents bad predictions from being fed back to the model, so the model never learns to recover from mistakes. Using *curriculum learning* can help you to stabilize the model's open-loop output.\n",
    "\n",
    "------\n",
    "\n",
    "*From Machinelearningmastery:*\n",
    "\n",
    "## Teacher-forcing\n",
    "**Teacher-forcing** is the method of feeding the input of the current step with the output of the previous step (`next_char`).\n",
    "\n",
    "It is a fast and effective way to train a recurrent neural network that uses output from prior time steps as input to the model.\n",
    "\n",
    "But, the approach can also result in models that may be fragile or limited when used in practice when the generated sequences vary from what was seen by the model during training.\n",
    "\n",
    "## Curriculum Learning\n",
    "\n",
    "Gradually force the model during training to deal with its own mistakes, as it would have to during inference.\n",
    "\n",
    "Basically, randomly choosing to use the ground truth output or the generated output from the previous time step as input for the current time step.\n",
    "\n",
    "The curriculum changes over time in what is called scheduled sampling where the procedure starts at forced learning and slowly decreases the probability of a forced input over the training epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
